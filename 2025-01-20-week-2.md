---
layout: post
title: "First Prototype Running"
date: 2025-01-20
week: 2
read_time: "6 min"
summary: "Got a barebones prototype working end-to-end. The data pipeline is janky but functional. Discovered three assumptions that were completely wrong."
tags: [prototype, data, learnings]
---

It runs. Badly, slowly, and with zero error handling — but it runs end-to-end. That milestone felt bigger than it probably should have.

## The Pipeline

Got the three-stage architecture from last week actually wired together. Input feeds into processing, processing produces output. The data flow is hacky — I'm hardcoding paths that should be config — but the structure is visible now.

```bash
# Quick sanity-check run
python run.py --input sample.csv --mode debug
# Output: 47 records processed, 3 skipped, 0 errors
# Time: 2.3s (target: <0.5s — we have work to do)
```

## Wrong Assumptions

Three things I believed at the start of the week that turned out to be false:

- **Input data is consistent.** It is not. Three different date formats in the same column, no validation, nulls in required fields.
- **The processing step is the bottleneck.** Actually, parsing is. Need to profile properly before optimizing anything.
- **The output format is simple.** Stakeholders want three different views of the same data. Back to the drawing board on that component.

## What's Holding

The core data model seems sound. I refactored it once already but the second version feels stable — the entities map cleanly to the problem domain.

## Next Week

Fix the input parsing to handle all three date formats. Write some actual tests. Resist the urge to redesign the whole thing before it's even working.

---

**Hours logged:** ~26 hrs &nbsp;·&nbsp; **Biggest time sink:** chasing a bug that was a typo
